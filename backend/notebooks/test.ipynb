{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489958e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepagents import create_deep_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from  langgraph.checkpoint.memory import MemorySaver\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d8d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from tavily import TavilyClient\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "@tool\n",
    "def internet_search(\n",
    "    query: str,\n",
    "    max_results: int = 5,\n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\",\n",
    "    include_raw_content: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Search the internet for information.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query string\n",
    "        max_results: The maximum number of search results to return\n",
    "\n",
    "        \n",
    "    Returns:\n",
    "        Search results as a dict\n",
    "    \"\"\"\n",
    "    return tavily_client.search(\n",
    "        query,\n",
    "        max_results=max_results,\n",
    "        include_raw_content=include_raw_content,\n",
    "        topic=topic,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c570a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "        api_key=os.environ.get(\"OLLAMA_API_KEY\"),\n",
    "        model=\"gpt-oss:120b\",\n",
    "        base_url=\"https://api.ollama.com\",\n",
    "        temperature=0.0,\n",
    "    )\n",
    "# llm = ChatOllama(\n",
    "#     model=\"gpt-oss:120b-cloud\", # Use the model you pulled\n",
    "#     base_url=\"http://localhost:11434\", # Point to the local Ollama server\n",
    "#     temperature=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7e3682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = open(\"../promt.md\",\"r\",encoding=\"utf8\").read()\n",
    "agent_graph = create_deep_agent(\n",
    "    model=llm,\n",
    "    tools=[internet_search],\n",
    "    # system_prompt=prompt,\n",
    "    checkpointer=MemorySaver(), # This enables Short-Term Chat Memory\n",
    "    # response_format={\"content\":\"answer of asked response\",\"tools_used\":[\"tool1\",\"tool2\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66456b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"session_1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d992db",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = agent_graph.stream(\n",
    "    {\"messages\": [(\"user\", \"write a plan to search who is prime minister of india using given tools\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cedb7e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m final_answer = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mevents\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event:\n\u001b[32m      4\u001b[39m         last_msg = event[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'events' is not defined"
     ]
    }
   ],
   "source": [
    "final_answer = \"\"\n",
    "for event in events:\n",
    "    if \"messages\" in event:\n",
    "        last_msg = event[\"messages\"][-1]\n",
    "        # Filter out intermediate tool calls, show only AI response\n",
    "        if last_msg.type == \"ai\" and not last_msg.tool_calls:\n",
    "            final_answer = last_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d22e5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Plan for Finding the Current Prime Minister of India Using the Available Tools**\n",
      "\n",
      "| Step | Action | Tool to Use | Reason |\n",
      "|------|--------|-------------|--------|\n",
      "| 1 | Formulate a concise search query that directly asks for the current Prime Minister of India. | `internet_search` | The web is the most up‑to‑date source for political office holders. |\n",
      "| 2 | Run the internet search with the query “who is the prime minister of India?” (or “current prime minister of India”). Request at least a few results (e.g., `max_results: 3`) and ask for raw content so the answer text is returned. | `internet_search` ( `query: \"who is the prime minister of India?\"`, `max_results: 3`, `include_raw_content: true` ) | This returns snippets from reliable sites (government pages, major news outlets, Wikipedia) that contain the name. |\n",
      "| 3 | Inspect the returned results and locate the line or snippet that explicitly states the name of the Prime Minister. | – (manual inspection of the returned JSON) | The name will appear in a sentence such as “Narendra Modi is the Prime Minister of India…”. |\n",
      "| 4 | (Optional) Verify the answer against a secondary source to reduce the chance of outdated information. | Run a second `internet_search` with the same query but request a different set of sources, or search a trusted site like “https://www.pmindia.gov.in”. | A second independent source confirms the result. |\n",
      "| 5 | (Optional) If a local offline reference (e.g., a cached Wikipedia dump) exists on the filesystem, locate it with `glob` or `grep` and extract the same information to double‑check. | `glob` (pattern: `**/india*prime*`) → `read_file` → `grep` (pattern: \"Prime Minister\") | Provides a fallback if internet access is unavailable. |\n",
      "| 6 | Consolidate the findings and present the name of the current Prime Minister of India to the user. | – (final output) | Gives the definitive answer based on the gathered evidence. |\n",
      "\n",
      "**Summary of the Core Workflow**\n",
      "\n",
      "1. **Search** → `internet_search` with a clear query.  \n",
      "2. **Extract** → Read the returned snippets and pick the name.  \n",
      "3. **Validate** (optional) → Run a second search or check a local file.  \n",
      "4. **Report** → Return the name to the user.\n",
      "\n",
      "This plan uses the tools provided, keeps the steps minimal, and includes optional verification steps for robustness.\n"
     ]
    }
   ],
   "source": [
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773aaa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] \n",
      "\n",
      "[('8b05af2f-1c49-9939-881c-8b1f772aaef0', '__no_writes__', None)] \n",
      "\n",
      "[('89a09f9d-5eae-feec-007e-12fd708c2038', 'messages', [AIMessage(content='The Prime Minister of India is **Narendra\\u202fModi**. He has been in office since May\\u202f2014.', additional_kwargs={}, response_metadata={'model': 'gpt-oss:120b', 'created_at': '2026-02-14T07:30:48.192643049Z', 'done': True, 'done_reason': 'stop', 'total_duration': 799386011, 'load_duration': None, 'prompt_eval_count': 4528, 'prompt_eval_duration': None, 'eval_count': 75, 'eval_duration': None, 'logprobs': None, 'model_name': 'gpt-oss:120b', 'model_provider': 'ollama'}, id='lc_run--019c5b0f-245b-7dc1-85f4-8b5d4ed14433-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 4528, 'output_tokens': 75, 'total_tokens': 4603})]), ('89a09f9d-5eae-feec-007e-12fd708c2038', 'branch:to:TodoListMiddleware.after_model', None)] \n",
      "\n",
      "[('2dc18c99-074e-6850-70a5-ac507831fd5c', 'branch:to:model', None)] \n",
      "\n",
      "[('66bfe251-5ca3-bf64-0c84-3441829190c9', 'messages', Overwrite(value=[HumanMessage(content='who is pm of india?', additional_kwargs={}, response_metadata={}, id='ffc18818-10fd-406e-ae01-073879a244b7')])), ('66bfe251-5ca3-bf64-0c84-3441829190c9', 'branch:to:SummarizationMiddleware.before_model', None)] \n",
      "\n",
      "[('498ebcb5-111a-93fd-2c48-f6453f425350', 'messages', [['user', 'who is pm of india?']]), ('498ebcb5-111a-93fd-2c48-f6453f425350', 'branch:to:PatchToolCallsMiddleware.before_agent', None)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in agent_graph.checkpointer.list(config):\n",
    "    print(i[4],\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legalgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
